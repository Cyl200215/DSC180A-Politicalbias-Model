{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the cleaning function to handle non-string values\n",
    "def clean_text(text):\n",
    "    # Check if the text is a string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags and non-alphanumeric characters\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# # Reapply the cleaning function\n",
    "# data['Statement_clean'] = data['statement'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"politifact_plus_data.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, test, validation data\n",
    "train_data = pd.read_csv(\"train2.tsv\",sep='\\t', header=None)\n",
    "test_data = pd.read_csv(\"test2.tsv\",sep='\\t', header=None)\n",
    "val_data = pd.read_csv(\"val2.tsv\",sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename all the columns\n",
    "train_data.rename({1: 'id', 2: 'label', 3: 'statement', 4: 'subject', 5: 'speaker', 6: 'job-title',\n",
    "           7: 'state_info', 8: 'party_affiliation', 9: 'barely_true_counts', 10: 'false_counts',\n",
    "           11: 'half_true_counts', 12: 'mostly_true_counts', 13: 'pants_on_fire_counts', 14: 'context',\n",
    "           15: 'justification'\n",
    "          }, axis = 1, inplace = True)\n",
    "test_data.rename({1: 'id', 2: 'label', 3: 'statement', 4: 'subject', 5: 'speaker', 6: 'job-title',\n",
    "           7: 'state_info', 8: 'party_affiliation', 9: 'barely_true_counts', 10: 'false_counts',\n",
    "           11: 'half_true_counts', 12: 'mostly_true_counts', 13: 'pants_on_fire_counts', 14: 'context',\n",
    "           15: 'justification'\n",
    "          }, axis = 1, inplace = True)\n",
    "val_data.rename({1: 'id', 2: 'label', 3: 'statement', 4: 'subject', 5: 'speaker', 6: 'job-title',\n",
    "           7: 'state_info', 8: 'party_affiliation', 9: 'barely_true_counts', 10: 'false_counts',\n",
    "           11: 'half_true_counts', 12: 'mostly_true_counts', 13: 'pants_on_fire_counts', 14: 'context',\n",
    "           15: 'justification'\n",
    "          }, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data, eliminate null values and clean texts\n",
    "train_data['Statement_clean'] = train_data['statement'].apply(clean_text)\n",
    "# train_data['Justification_clean'] = train_data['justification'].apply(clean_text)\n",
    "train_data = train_data[~train_data['label'].isna()]\n",
    "\n",
    "test_data['Statement_clean'] = test_data['statement'].apply(clean_text)\n",
    "# test_data['Justification_clean'] = test_data['justification'].apply(clean_text)\n",
    "test_data = test_data[~test_data['label'].isna()]\n",
    "\n",
    "val_data['Statement_clean'] = val_data['statement'].apply(clean_text)\n",
    "# val_data['Justification_clean'] = val_data['justification'].apply(clean_text)\n",
    "val_data = val_data[~val_data['label'].isna()]\n",
    "\n",
    "# Simple preprocessing\n",
    "train_data['text'] = train_data['Statement_clean']  # Combining Justification + statement\n",
    "train_data = train_data[['text', 'label']]  \n",
    "\n",
    "val_data['text'] = val_data['Statement_clean'] \n",
    "val_data = val_data[['text', 'label']] \n",
    "\n",
    "test_data['text'] = test_data['Statement_clean']\n",
    "test_data = test_data[['text', 'label']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.3.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'classifier.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'classifier.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Cyl20\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 625/625 [40:32<00:00,  3.89s/it] \n",
      "100%|██████████| 625/625 [39:37<00:00,  3.80s/it]\n",
      "100%|██████████| 625/625 [40:55<00:00,  3.93s/it]\n",
      "100%|██████████| 625/625 [15:03<00:00,  1.45s/it]\n",
      "100%|██████████| 161/161 [03:53<00:00,  1.45s/it]\n",
      "c:\\Users\\Cyl20\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Cyl20\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Cyl20\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 159/159 [03:45<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      " {'0': {'precision': 0.2358490566037736, 'recall': 0.03221649484536082, 'f1-score': 0.05668934240362811, 'support': 776.0}, '1': {'precision': 0.2718978102189781, 'recall': 0.14723320158102768, 'f1-score': 0.191025641025641, 'support': 1012.0}, '2': {'precision': 0.23503569467325644, 'recall': 0.42502482621648463, 'f1-score': 0.3026874115983027, 'support': 1007.0}, '3': {'precision': 0.2138364779874214, 'recall': 0.3497942386831276, 'f1-score': 0.2654176424668228, 'support': 972.0}, '4': {'precision': 0.6666666666666666, 'recall': 0.004901960784313725, 'f1-score': 0.009732360097323601, 'support': 408.0}, '5': {'precision': 0.2017167381974249, 'recall': 0.22787878787878788, 'f1-score': 0.21400113830392717, 'support': 825.0}, 'accuracy': 0.2264, 'macro avg': {'precision': 0.3041670740579202, 'recall': 0.19784158499818372, 'f1-score': 0.17325892264927423, 'support': 5000.0}, 'weighted avg': {'precision': 0.26822515240375044, 'recall': 0.2264, 'f1-score': 0.19612455848017094, 'support': 5000.0}}\n",
      "\n",
      "Validation Set Evaluation:\n",
      " {'0': {'precision': 0.11538461538461539, 'recall': 0.012658227848101266, 'f1-score': 0.022813688212927757, 'support': 237.0}, '1': {'precision': 0.3106060606060606, 'recall': 0.155893536121673, 'f1-score': 0.20759493670886076, 'support': 263.0}, '2': {'precision': 0.24183006535947713, 'recall': 0.4475806451612903, 'f1-score': 0.314002828854314, 'support': 248.0}, '3': {'precision': 0.21256038647342995, 'recall': 0.350597609561753, 'f1-score': 0.2646616541353384, 'support': 251.0}, '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 116.0}, '5': {'precision': 0.13043478260869565, 'recall': 0.1952662721893491, 'f1-score': 0.15639810426540285, 'support': 169.0}, 'accuracy': 0.21495327102803738, 'macro avg': {'precision': 0.1684693184053798, 'recall': 0.1936660484803611, 'f1-score': 0.16091186869614063, 'support': 1284.0}, 'weighted avg': {'precision': 0.19034699319345685, 'recall': 0.21495327102803738, 'f1-score': 0.17970277945918, 'support': 1284.0}}\n",
      "\n",
      "Test Set Evaluation:\n",
      " {'0': {'precision': 0.07692307692307693, 'recall': 0.009433962264150943, 'f1-score': 0.01680672268907563, 'support': 212.0}, '1': {'precision': 0.26811594202898553, 'recall': 0.14859437751004015, 'f1-score': 0.19121447028423771, 'support': 249.0}, '2': {'precision': 0.2292134831460674, 'recall': 0.3849056603773585, 'f1-score': 0.28732394366197184, 'support': 265.0}, '3': {'precision': 0.18434343434343434, 'recall': 0.3029045643153527, 'f1-score': 0.22919937205651492, 'support': 241.0}, '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 92.0}, '5': {'precision': 0.1891891891891892, 'recall': 0.23557692307692307, 'f1-score': 0.20985010706638116, 'support': 208.0}, 'accuracy': 0.20757695343330704, 'macro avg': {'precision': 0.15796418760512557, 'recall': 0.18023591459063756, 'f1-score': 0.15573243595969688, 'support': 1267.0}, 'weighted avg': {'precision': 0.17962766687824513, 'recall': 0.20757695343330704, 'f1-score': 0.1785336577085313, 'support': 1267.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize label encoder\n",
    "label_encoder1 = LabelEncoder()\n",
    "label_encoder2 = LabelEncoder()\n",
    "label_encoder3 = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "train_data['label'] = label_encoder1.fit_transform(train_data['label'])\n",
    "val_data['label'] = label_encoder2.fit_transform(val_data['label'])\n",
    "test_data['label'] = label_encoder3.fit_transform(test_data['label'])\n",
    "\n",
    "# Split the data (with encoded labels)\n",
    "train_texts = train_data['text'][:5000]\n",
    "val_texts = val_data['text'][:5000]\n",
    "test_texts = test_data['text'][:5000]\n",
    "\n",
    "train_labels = train_data['label'][:5000]\n",
    "val_labels = val_data['label'][:5000]\n",
    "test_labels = test_data['label'][:5000]\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "train_encodings = tokenize_function(train_texts.tolist())\n",
    "val_encodings = tokenize_function(val_texts.tolist())\n",
    "test_encodings = tokenize_function(test_texts.tolist())\n",
    "\n",
    "# Dataset class\n",
    "class PoliticalBiasDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert to dataset format\n",
    "train_dataset = PoliticalBiasDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = PoliticalBiasDataset(val_encodings, val_labels.tolist())\n",
    "test_dataset = PoliticalBiasDataset(test_encodings, test_labels.tolist())\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(train_data['label'].unique()))\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 3)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights for training loss\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(outputs.logits.view(-1, len(train_data['label'].unique())), labels.view(-1))\n",
    "        # outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        # loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "        references.extend(labels.tolist())\n",
    "    return classification_report(references, predictions, output_dict=True)\n",
    "\n",
    "# Evaluate on training, validation, and test sets\n",
    "train_report = evaluate_model(model, train_loader)\n",
    "val_report = evaluate_model(model, val_loader)\n",
    "test_report = evaluate_model(model, test_loader) \n",
    "\n",
    "print(\"Training Set Evaluation:\\n\", train_report)\n",
    "print(\"\\nValidation Set Evaluation:\\n\", val_report)\n",
    "print(\"\\nTest Set Evaluation:\\n\", test_report)\n",
    "\n",
    "# Classification report\n",
    "# print(classification_report(references, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with Pytorch (Two BERT models in a Siamese Network)\n",
    "\n",
    "**Basic model architecture and ideas from https://github.com/manideep2510/siamese-BERT-fake-news-detection-LIAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data, eliminate null values and clean texts\n",
    "train_data['Statement_clean'] = train_data['statement'].apply(clean_text)\n",
    "train_data['Justification_clean'] = train_data['justification'].apply(clean_text)\n",
    "train_data = train_data[~train_data['label'].isna()]\n",
    "\n",
    "test_data['Statement_clean'] = test_data['statement'].apply(clean_text)\n",
    "test_data['Justification_clean'] = test_data['justification'].apply(clean_text)\n",
    "test_data = test_data[~test_data['label'].isna()]\n",
    "\n",
    "val_data['Statement_clean'] = val_data['statement'].apply(clean_text)\n",
    "val_data['Justification_clean'] = val_data['justification'].apply(clean_text)\n",
    "val_data = val_data[~val_data['label'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Statement_clean'] = new_data['statement'].apply(clean_text)\n",
    "new_data['Justification_clean'] = new_data['justification'].apply(clean_text)\n",
    "new_data = new_data[~new_data['label'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = train_data[['label', 'Statement_clean', 'Justification_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = new_data[['label', 'Statement_clean', 'Justification_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Statement_clean</th>\n",
       "      <th>Justification_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>says the annies list political group supports ...</td>\n",
       "      <td>thats a premise that he fails to back up annie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>half-true</td>\n",
       "      <td>when did the decline of coal start it started ...</td>\n",
       "      <td>surovell said the decline of coal started when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>hillary clinton agrees with john mccain by vot...</td>\n",
       "      <td>obama said he would have voted against the ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>health care reform legislation is likely to ma...</td>\n",
       "      <td>the release may have a point that mikulskis co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>half-true</td>\n",
       "      <td>the economic turnaround started at the end of ...</td>\n",
       "      <td>crist said that the economic turnaround starte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11124</th>\n",
       "      <td>pants-fire</td>\n",
       "      <td>chemtrails are being put into the atmosphere a...</td>\n",
       "      <td>atmospheric chemists and geochemists dispute t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11125</th>\n",
       "      <td>pants-fire</td>\n",
       "      <td>there is no biden presidency the real biden wa...</td>\n",
       "      <td>president joe biden and former secretary of st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11126</th>\n",
       "      <td>pants-fire</td>\n",
       "      <td>video shows someone impersonating joe biden in...</td>\n",
       "      <td>if this video shows anything its that biden  h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11127</th>\n",
       "      <td>pants-fire</td>\n",
       "      <td>el ceo de pfizer es un lagarto demonio</td>\n",
       "      <td>el ceo de pfizer albert bourla es humanola teo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128</th>\n",
       "      <td>pants-fire</td>\n",
       "      <td>a pentagon plan and a cdc poster proves that a...</td>\n",
       "      <td>an unclassified document from the defense depa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11129 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label                                    Statement_clean  \\\n",
       "0            false  says the annies list political group supports ...   \n",
       "1        half-true  when did the decline of coal start it started ...   \n",
       "2      mostly-true  hillary clinton agrees with john mccain by vot...   \n",
       "3            false  health care reform legislation is likely to ma...   \n",
       "4        half-true  the economic turnaround started at the end of ...   \n",
       "...            ...                                                ...   \n",
       "11124   pants-fire  chemtrails are being put into the atmosphere a...   \n",
       "11125   pants-fire  there is no biden presidency the real biden wa...   \n",
       "11126   pants-fire  video shows someone impersonating joe biden in...   \n",
       "11127   pants-fire             el ceo de pfizer es un lagarto demonio   \n",
       "11128   pants-fire  a pentagon plan and a cdc poster proves that a...   \n",
       "\n",
       "                                     Justification_clean  \n",
       "0      thats a premise that he fails to back up annie...  \n",
       "1      surovell said the decline of coal started when...  \n",
       "2      obama said he would have voted against the ame...  \n",
       "3      the release may have a point that mikulskis co...  \n",
       "4      crist said that the economic turnaround starte...  \n",
       "...                                                  ...  \n",
       "11124  atmospheric chemists and geochemists dispute t...  \n",
       "11125  president joe biden and former secretary of st...  \n",
       "11126  if this video shows anything its that biden  h...  \n",
       "11127  el ceo de pfizer albert bourla es humanola teo...  \n",
       "11128  an unclassified document from the defense depa...  \n",
       "\n",
       "[11129 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat([df1, df2], ignore_index=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Network\n",
    "class SiameseBERTNetwork(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(SiameseBERTNetwork, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Since we are concatenating the outputs, the input features to the linear layer are doubled\n",
    "        self.classifier = nn.Linear(768 * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        output1 = self.bert(input_ids1, attention_mask=attention_mask1)\n",
    "        output2 = self.bert(input_ids2, attention_mask=attention_mask2)\n",
    "\n",
    "        pooled_output1 = self.dropout(output1.pooler_output)\n",
    "        pooled_output2 = self.dropout(output2.pooler_output)\n",
    "\n",
    "        # Concatenate the outputs\n",
    "        concat_output = torch.cat((pooled_output1, pooled_output2), dim=1)\n",
    "\n",
    "        # Pass through the classifier\n",
    "        logits = self.classifier(concat_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset class\n",
    "class TextPairDataset(Dataset):\n",
    "    def __init__(self, text_pairs, labels, tokenizer, max_len=128):\n",
    "        self.text_pairs = text_pairs\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        statement, justification = self.text_pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the statement\n",
    "        encoded_statement = self.tokenizer(\n",
    "            statement, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids_statement = encoded_statement['input_ids'].squeeze(0)\n",
    "        attention_mask_statement = encoded_statement['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Tokenize the justification\n",
    "        encoded_justification = self.tokenizer(\n",
    "            justification, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids_justification = encoded_justification['input_ids'].squeeze(0)\n",
    "        attention_mask_justification = encoded_justification['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Return all the elements as separate items\n",
    "        return input_ids_statement, attention_mask_statement, input_ids_justification, attention_mask_justification, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "def prepare_data(data):\n",
    "    # Mapping labels to numerical values\n",
    "    label_mapping = {'false': 0, 'half-true': 1, 'mostly-true': 2, 'true': 3, 'barely-true': 4, 'pants-fire': 5}\n",
    "    data['label'] = data['label'].map(label_mapping)\n",
    "    \n",
    "    text_pairs = data[['Statement_clean', 'Justification_clean']].values.tolist()\n",
    "    labels = data['label'].values.tolist()\n",
    "\n",
    "    return text_pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, validation, and test sets\n",
    "train_text_pairs, train_labels = prepare_data(train_data)\n",
    "val_text_pairs, val_labels = prepare_data(val_data)\n",
    "test_text_pairs, test_labels = prepare_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Creating instances of TextPairDataset\n",
    "train_dataset = TextPairDataset(train_text_pairs, train_labels, tokenizer)\n",
    "val_dataset = TextPairDataset(val_text_pairs, val_labels, tokenizer)\n",
    "test_dataset = TextPairDataset(test_text_pairs, test_labels, tokenizer)\n",
    "\n",
    "# Creating DataLoader instances\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyl20\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 1392/1392 [29:32<00:00,  1.27s/it]\n",
      "100%|██████████| 161/161 [01:10<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 1.7269615991704765\n",
      "Validation Report: {'0': {'precision': 0.2876712328767123, 'recall': 0.3193916349809886, 'f1-score': 0.3027027027027027, 'support': 263.0}, '1': {'precision': 0.2040169133192389, 'recall': 0.7782258064516129, 'f1-score': 0.323283082077052, 'support': 248.0}, '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 251.0}, '3': {'precision': 1.0, 'recall': 0.005917159763313609, 'f1-score': 0.011764705882352941, 'support': 169.0}, '4': {'precision': 0.20930232558139536, 'recall': 0.0379746835443038, 'f1-score': 0.0642857142857143, 'support': 237.0}, '5': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 116.0}, 'accuracy': 0.2235202492211838, 'macro avg': {'precision': 0.2834984119628911, 'recall': 0.19025154745670314, 'f1-score': 0.11700603415797033, 'support': 1284.0}, 'weighted avg': {'precision': 0.2685812927667736, 'recall': 0.2235202492211838, 'f1-score': 0.1378574491789343, 'support': 1284.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1392/1392 [29:38<00:00,  1.28s/it]\n",
      "100%|██████████| 161/161 [01:10<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.6068484474016331\n",
      "Validation Report: {'0': {'precision': 0.2849162011173184, 'recall': 0.19391634980988592, 'f1-score': 0.23076923076923075, 'support': 263.0}, '1': {'precision': 0.2680851063829787, 'recall': 0.2540322580645161, 'f1-score': 0.26086956521739135, 'support': 248.0}, '2': {'precision': 0.25513196480938416, 'recall': 0.3466135458167331, 'f1-score': 0.2939189189189189, 'support': 251.0}, '3': {'precision': 0.22676579925650558, 'recall': 0.3609467455621302, 'f1-score': 0.27853881278538817, 'support': 169.0}, '4': {'precision': 0.25877192982456143, 'recall': 0.2489451476793249, 'f1-score': 0.2537634408602151, 'support': 237.0}, '5': {'precision': 0.4375, 'recall': 0.1206896551724138, 'f1-score': 0.1891891891891892, 'support': 116.0}, 'accuracy': 0.26090342679127726, 'macro avg': {'precision': 0.2885285002317914, 'recall': 0.2541906170175007, 'f1-score': 0.251174859623389, 'support': 1284.0}, 'weighted avg': {'precision': 0.2771484095691273, 'recall': 0.26090342679127726, 'f1-score': 0.25570292001979505, 'support': 1284.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1392/1392 [29:39<00:00,  1.28s/it]\n",
      "100%|██████████| 161/161 [01:10<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 1.2711485921908383\n",
      "Validation Report: {'0': {'precision': 0.25316455696202533, 'recall': 0.22813688212927757, 'f1-score': 0.24, 'support': 263.0}, '1': {'precision': 0.2422680412371134, 'recall': 0.3790322580645161, 'f1-score': 0.29559748427672955, 'support': 248.0}, '2': {'precision': 0.2747747747747748, 'recall': 0.24302788844621515, 'f1-score': 0.25792811839323465, 'support': 251.0}, '3': {'precision': 0.2372093023255814, 'recall': 0.30177514792899407, 'f1-score': 0.26562499999999994, 'support': 169.0}, '4': {'precision': 0.22702702702702704, 'recall': 0.17721518987341772, 'f1-score': 0.1990521327014218, 'support': 237.0}, '5': {'precision': 0.4594594594594595, 'recall': 0.14655172413793102, 'f1-score': 0.22222222222222224, 'support': 116.0}, 'accuracy': 0.2531152647975078, 'macro avg': {'precision': 0.2823171936309969, 'recall': 0.2459565150967253, 'f1-score': 0.24673749293226807, 'support': 1284.0}, 'weighted avg': {'precision': 0.2669971152429994, 'recall': 0.2531152647975078, 'f1-score': 0.24845162931880493, 'support': 1284.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1392/1392 [29:43<00:00,  1.28s/it]\n",
      "100%|██████████| 161/161 [01:10<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.6394956305853209\n",
      "Validation Report: {'0': {'precision': 0.2696245733788396, 'recall': 0.30038022813688214, 'f1-score': 0.2841726618705036, 'support': 263.0}, '1': {'precision': 0.22580645161290322, 'recall': 0.28225806451612906, 'f1-score': 0.2508960573476703, 'support': 248.0}, '2': {'precision': 0.2947761194029851, 'recall': 0.3147410358565737, 'f1-score': 0.3044315992292871, 'support': 251.0}, '3': {'precision': 0.28368794326241137, 'recall': 0.23668639053254437, 'f1-score': 0.25806451612903225, 'support': 169.0}, '4': {'precision': 0.21008403361344538, 'recall': 0.2109704641350211, 'f1-score': 0.21052631578947367, 'support': 237.0}, '5': {'precision': 0.4411764705882353, 'recall': 0.12931034482758622, 'f1-score': 0.2, 'support': 116.0}, 'accuracy': 0.25934579439252337, 'macro avg': {'precision': 0.28752593197647, 'recall': 0.24572442133412276, 'f1-score': 0.2513485250609945, 'support': 1284.0}, 'weighted avg': {'precision': 0.2724374748713033, 'recall': 0.25934579439252337, 'f1-score': 0.25707134249893104, 'support': 1284.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1392/1392 [29:40<00:00,  1.28s/it]\n",
      "100%|██████████| 161/161 [01:10<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.24794589627625144\n",
      "Validation Report: {'0': {'precision': 0.24642857142857144, 'recall': 0.2623574144486692, 'f1-score': 0.2541436464088398, 'support': 263.0}, '1': {'precision': 0.23765432098765432, 'recall': 0.31048387096774194, 'f1-score': 0.2692307692307692, 'support': 248.0}, '2': {'precision': 0.3076923076923077, 'recall': 0.3346613545816733, 'f1-score': 0.3206106870229008, 'support': 251.0}, '3': {'precision': 0.271523178807947, 'recall': 0.24260355029585798, 'f1-score': 0.25625000000000003, 'support': 169.0}, '4': {'precision': 0.22641509433962265, 'recall': 0.20253164556962025, 'f1-score': 0.21380846325167038, 'support': 237.0}, '5': {'precision': 0.45454545454545453, 'recall': 0.1724137931034483, 'f1-score': 0.25000000000000006, 'support': 116.0}, 'accuracy': 0.26401869158878505, 'macro avg': {'precision': 0.29070982130025963, 'recall': 0.2541752714945018, 'f1-score': 0.26067392765236336, 'support': 1284.0}, 'weighted avg': {'precision': 0.275120578213184, 'recall': 0.26401869158878505, 'f1-score': 0.2625086822493377, 'support': 1284.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:09<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation Report: {'0': {'precision': 0.2901023890784983, 'recall': 0.3413654618473896, 'f1-score': 0.31365313653136534, 'support': 249.0}, '1': {'precision': 0.23547400611620795, 'recall': 0.29056603773584905, 'f1-score': 0.26013513513513514, 'support': 265.0}, '2': {'precision': 0.29642857142857143, 'recall': 0.34439834024896265, 'f1-score': 0.31861804222648754, 'support': 241.0}, '3': {'precision': 0.2929936305732484, 'recall': 0.22115384615384615, 'f1-score': 0.2520547945205479, 'support': 208.0}, '4': {'precision': 0.26857142857142857, 'recall': 0.22169811320754718, 'f1-score': 0.24289405684754523, 'support': 212.0}, '5': {'precision': 0.42857142857142855, 'recall': 0.16304347826086957, 'f1-score': 0.23622047244094488, 'support': 92.0}, 'accuracy': 0.2786108918705604, 'macro avg': {'precision': 0.3020235757232305, 'recall': 0.26370421290907736, 'f1-score': 0.2705959396170044, 'support': 1267.0}, 'weighted avg': {'precision': 0.2868064575063748, 'recall': 0.2786108918705604, 'f1-score': 0.27582921133403693, 'support': 1267.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop \n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "\n",
    "        # Unpack the data from the dataset\n",
    "        input_ids_statement, attention_mask_statement, input_ids_justification, attention_mask_justification, labels = [b.to(device) for b in batch]\n",
    "        # # Unpack the data from the dataset\n",
    "        # input_ids_statement = batch['input_ids_statement'].to(device)\n",
    "        # attention_mask_statement = batch['attention_mask_statement'].to(device)\n",
    "        # input_ids_justification = batch['input_ids_justification'].to(device)\n",
    "        # attention_mask_justification = batch['attention_mask_justification'].to(device)\n",
    "        # labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids_statement, attention_mask_statement, input_ids_justification, attention_mask_justification)\n",
    "        \n",
    "        loss = loss_function(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            # Unpack the data from the dataset\n",
    "            input_ids_statement, attention_mask_statement, input_ids_justification, attention_mask_justification, labels = [b.to(device) for b in batch]\n",
    "            logits = model(input_ids_statement, attention_mask_statement, input_ids_justification, attention_mask_justification)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "            batch_predictions = np.argmax(logits, axis=1)\n",
    "            predictions.extend(batch_predictions)\n",
    "            true_labels.extend(label_ids)\n",
    "\n",
    "    return classification_report(true_labels, predictions, output_dict=True)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# Model, Optimizer, and Scheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 6 \n",
    "model = SiameseBERTNetwork(num_labels)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "    val_report = evaluate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch}, Training Loss: {train_loss}\")\n",
    "    print(f\"Validation Report: {val_report}\")\n",
    "\n",
    "# Evaluate on the test set \n",
    "test_report = evaluate(model, test_loader, device)\n",
    "print(f\"Test Set Evaluation Report: {test_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
